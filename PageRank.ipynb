{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PageRank algorithm coded without any library based on a set of html pages in a sub-folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Construct webpages graph from pages in a directory\n",
    "webpages = {}\n",
    "for element in os.listdir('pages'): #Loop on all elements of toyset's folder\n",
    "    webpages[element] = len(webpages) + 1\n",
    "\n",
    "graph = open(\"./graph.txt\", \"w\") #Create graph.txt file\n",
    "for wb in webpages:\n",
    "    f=open(\"pages/\" + wb, \"r\", encoding=\"utf8\")\n",
    "    if f.mode == 'r':\n",
    "        content = f.read()\n",
    "        datas = re.findall(r'a href=\"[\\S]*.html[\\S]', content) #find all links on the webpage\n",
    "        for e in datas:\n",
    "            graph.write(str(webpages[wb]) + \" \" + str(webpages[e.split('\"')[1]]) + \"\\r\") #write in graph.txt the edge between the two pages\n",
    "    f.close()\n",
    "graph.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create set of functions to deal with matrices\n",
    "def matricial_product(M,PI):\n",
    "    result = [0.0 for i in range(len(PI))]\n",
    "    for e in M:\n",
    "        i, j, value = e.split(' ')\n",
    "        i = int(i) - 1\n",
    "        j = int(j) - 1\n",
    "        result[int(i)] += float(value) * float(PI[int(j)])\n",
    "    return result\n",
    "\n",
    "def scalar_matrix_product(scalar, M):  \n",
    "    result = []  \n",
    "    for e in M:\n",
    "        args = e.split(' ')\n",
    "        args[2] = str(float(args[2]) * scalar)\n",
    "        result.append(' '.join(args))\n",
    "    return result\n",
    "    \n",
    "def get_norm1(A):\n",
    "    sum = 0\n",
    "    for e in A:\n",
    "        sum += abs(e)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRankAlgorithm(graph_path, beta, epsilon):\n",
    "    G = []\n",
    "    with open(graph_path, 'r')  as graph_file:\n",
    "        for line in graph_file.readlines(): #Open graph file, read the lines and creates the graph in a frame of [(i,j),...]\n",
    "            args = line.split(' ')\n",
    "            G.append((int(args[0]), int(args[1])))\n",
    "\n",
    "    #Compute Mg matrix\n",
    "    #Start create a dictionnary in which the key is a node and the value is a list of the page with which there are one or more links from the key node.\n",
    "    graph_dictionnary = {}\n",
    "    for e in G:\n",
    "        if e[0] not in graph_dictionnary:\n",
    "            graph_dictionnary[e[0]] = []\n",
    "        if e[1] not in graph_dictionnary[e[0]] and e[1] != e[0]:\n",
    "            graph_dictionnary[e[0]].append(e[1])\n",
    "\n",
    "    n = len(graph_dictionnary.keys())\n",
    "\n",
    "    #As there is no dead-ends we can suppose that there is at least an edge starting from each point (key value)\n",
    "    M = []\n",
    "    for i in range(n):\n",
    "        keys_i = [*graph_dictionnary][i]\n",
    "        for j in range(n):\n",
    "            keys_j = [*graph_dictionnary][j]\n",
    "            if (keys_i == keys_j):\n",
    "                continue\n",
    "            if keys_i in graph_dictionnary[keys_j]:\n",
    "                M.append(\" \".join([str(keys_i), str(keys_j), str(1/len(graph_dictionnary[keys_j]))]))\n",
    "\n",
    "    #Initialize pi vector\n",
    "    pi = []\n",
    "    pi.append([1/n for i in range(n)])\n",
    "    iteration = 0\n",
    "    while 1:\n",
    "        sparse_product = [beta * e + (1-beta)/n for e in matricial_product(M, pi[-1])]\n",
    "        pi.append(sparse_product)\n",
    "        if get_norm1([pi[-1][i] - pi[-2][i] for i in range(len(pi[-1]))]) < epsilon:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if iteration >= 100:\n",
    "            break\n",
    "\n",
    "    return pi[-1]"
   ]
  },
  {
   "source": [
    "## How to use algorithm : "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = PageRankAlgorithm2(\"./graph.txt\", 1, 0.1)\n",
    "max, min = 0, 1\n",
    "max_index, min_index = 0,0\n",
    "for i in range(len(vector)):\n",
    "    if vector[i] > max:\n",
    "        max = vector[i]\n",
    "        max_index = i\n",
    "    if vector[i] < min:\n",
    "        min = vector[i]\n",
    "        min_index = i\n",
    "    print(str(vector[i]) + \" --> \" + [*webpages][i])\n",
    "\n",
    "print()\n",
    "print(\"Max value : \" + [*webpages][max_index] + \" (\" + str(max) + \")\")\n",
    "print(\"Min value : \" + [*webpages][min_index] + \" (\" + str(min) + \")\")"
   ]
  },
  {
   "source": [
    "## A function if you want to delete dead-ends"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dead_ends(graph_path):\n",
    "    G = []\n",
    "    with open(graph_path, 'r')  as graph_file: #Load graph datas from txt files\n",
    "        for line in graph_file.readlines(): #Read all lines and recreate the graph in a frame of [(i,j),...]\n",
    "            args = line.split(' ')\n",
    "            G.append((int(args[0]), int(args[1])))\n",
    "\n",
    "    dead_ends = [-1] #Initialize a virtual dead-end to enter in the loop.\n",
    "    while len(dead_ends) > 0: #While we found dead-end at previous operation continue to remove dead-ends\n",
    "        start = [] #start will represent all the nodes that are the start of at least one arrow\n",
    "        destination = [] #destination will represent all the nodes that are the end of at least one arrow\n",
    "        for i, j in G:\n",
    "            if j not in destination:\n",
    "                destination.append(j)\n",
    "            if i not in start:\n",
    "                start.append(i)\n",
    "\n",
    "        dead_ends = [] #re-instanciate dead-end list, 0 dead-end found yet\n",
    "        for d in destination:\n",
    "            if d not in start: #if a node is a destination and not a start --> that's a dead-end\n",
    "                dead_ends.append(d)\n",
    "        \n",
    "        if len(dead_ends) == 0: #if there is no dead-end, the job is done !\n",
    "            break\n",
    "        \n",
    "        new_g = [] #else recompute the new graph without the dead-end point <=> without the arrows that go to the dead end points\n",
    "        for i, j in G:\n",
    "            if j in dead_ends:\n",
    "                continue\n",
    "            new_g.append((i,j))\n",
    "        G = new_g\n",
    "    return G"
   ]
  }
 ]
}